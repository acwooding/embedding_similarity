{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional analysis and data generation is done in separate notebooks. For details, see: \n",
    "* `Embedding IO.ipynb`\n",
    "* `Creating Neighbors from a Directory.ipynb`\n",
    "* `adaptedKendallTau.ipynb`\n",
    "* `Comparing All Neighbour Sets for Workshop Summary.ipynb`\n",
    "* TODO: `Comparing knn metrics.ipynb`: Create one that includes comparisons of knn-metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "module_location = \"/scratch/research/text_workshop_2018\"\n",
    "data_path = \"/scratch/research/text_workshop_2018/data/embeddings\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(module_location)\n",
    "\n",
    "from textproc import embedding as embed\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "from itertools import combinations as comb\n",
    "\n",
    "#Plotting stuff\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Vector Space Embeddings\n",
    "\n",
    "One of the goals of the text analysis workshop at TIMC (January 2018) was to test out ways to **compare vector space embeddings**. The goal of the comparison is to analyze the **similarity of the local neighbourhoods** of points in the embeddings as a way of determining if an embedding techinique is similar to **itself** under multiple applications to the same data (aka. stability of the embedding technique), and to evaluate the how similar in local structure of **different** embedding techniques are.\n",
    "\n",
    "Since this was a text analysis workshop, we wanted to compare the similarity between common text embedding techniques. We used the Yelp dataset (tokenized in a standard fashion) for all the comparisons. These were the techniques that we compared (so far):\n",
    "* PMI+SVD (factorization of the pointwise mutual information (PMI) matrix via truncated SVD)\n",
    "* fastext\n",
    "* eigenwords\n",
    "* GloVe\n",
    "* TODO: word2vec\n",
    "\n",
    "We proposed a way of comparing embeddings using **nearest neighbour structures** as follows:\n",
    "\n",
    "Given a point, $P$, compare the similarity of its k-nearest neighbours under two different embeddings using a set or partial order similarity metric (eg. jaccard) and call this the **local neighbourhood similarity score** of $P$. Then aggregate the local similarity scores of all the comparable points in the embeddings (say, using the mean) to produce a **(global) neighbourhood similarity score** between the two embeddings. This score will represent the similarity of two different embeddings of the same data.\n",
    "\n",
    "To test that this makes sense, we generated many different embeddings and created an infrastructure for easily computing the pairwise similarity of multiple different embeddings of the same dataset. The infrastructure can be found in the module `embedding.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Embeddings\n",
    "\n",
    "Go ahead and create an embedding of your favorite data via your favorite technique. To make your data ingestible by the similarity comparison infrastructure, save your embedding using the `save_embedding` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(embed.save_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example embedding created on the Yelp dataset using fasttext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#embedding_basefilename = \"yelp_pmi_svd_randomized\"\n",
    "embedding_basefilename = \"yelp_fastext_dim128_min_count1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "embedding, labels, metadata = embed.read_embedding(embedding_basefilename, data_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating k-nearest neighbors\n",
    "\n",
    "The easiest way to do this is to use `get_neighbors`, supplying the base filename of your embedding and a list of `run_numbers` you want to process. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(embed.get_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is the option of changing the number of neighbors that you want along with the metric you want to apply to compute distance between points in your vector space. We'll explore the effects of these choices in the notebook entitled `Comparing knn metrics.ipynb`.\n",
    "\n",
    "First, generate the 100 nearest neighbours under the cosine metric for the fasttext embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "embed.make_neighbors(embedding_basefilename, run_numbers=[0, 1], k=100, knn_metrics=['cosine'], data_path=data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a helpful piece of code that will allow you to look for all the embedding files in a directory that don't yet have have associated neighbor sets files for a given list of k-nearest neighbour metrics, and will generate the missing k-nearest neighbour sets in parallel in this accompanying notebook: `Creating Neighbors from a Directory.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neighbors_1, neighbors_metadata_1 = embed.read_neighbors(embedding_basefilename, metric='cosine',\n",
    "                                                         run_number=0, data_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_metadata_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`neighbors_1[0]` is a dict keyed by label (in this case words in the Yelp dataset) with values the list of the 100 nearest neighbours to that label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label = 'ottawa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neighbors_1[0][label][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`neighbors_1[1]` is a dict keyed by label (in this case words in the Yelp dataset) with values the list of the distance to the 100 nearest neighbours to that label. We don't use this in our analysis yet, but we plan to try some comparisons that may use the actual distances to compute the local similarity score in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_1[1][label][:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Single Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before comparing accross embedding techniques, let's do a single comparison, of two embeddings using the exact same parameters to test for stability and to get a feel for how we're computing similarity.\n",
    "\n",
    "We need another embedding and its nearest neighbors to do a comparison, so lets take a different embedding of the same data, under the same embedding technique with the same k-nearest neighbour metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neighbors_2, neighbors_metadata_2 = embed.read_neighbors(embedding_basefilename, metric='cosine',\n",
    "                                                         run_number=1, data_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_metadata_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neighbors_2[0][label][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity score of these neighbor lists are as follows (note that we omit the \"first\" neighbour itself in this calculation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Jaccard similarity: {}\".format(embed.list_similarity(label, neighbors_1[0][label][1:], neighbors_2[0][label][1:], how='jaccard')))\n",
    "print(\"Adapted Kendall-Tau similarity: {}\".format(embed.list_similarity(label, neighbors_1[0][label][1:], neighbors_2[0][label][1:], how='adapted-ktau')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adapted Kendall-Tau similarity being larger than the Jaccard similarity suggests that the neighbours that are in common are mostly in the same order.\n",
    "\n",
    "Let's check.\n",
    "\n",
    "First off, observe that most of the neighbours of \"ottawa\" don't actually match (in terms of exact position)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(neighbors_1[0][label][1:]) == np.array(neighbors_2[0][label][1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there is a lot of overlap in the first 10 neighbours, and even their order, even if they aren't in the exact same positions. We'll see that the adapted Kendall-Tau score will take this into account. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intersection = set(neighbors_1[0][label][1:11]).intersection(neighbors_2[0][label][1:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intersection_1 = [x for x in neighbors_1[0][label][1:] if x in intersection]\n",
    "intersection_2 = [x for x in neighbors_2[0][label][1:] if x in intersection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(intersection_1, intersection_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll continue to observe this difference between Jaccard and adapted Kendall-Tau throughout the rest of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Neighbours\n",
    "\n",
    "To compare neighbors between two embeddings, for each label, $w$, that belongs to both embeddings, we compare its neighbors by computing $s(n_1[w], n_2[w])$ where $n_i[w]$ are the neighbors of $w$ under the $i^{th}$ embedding ($i= 1, 2$), and $s$ is a similarity scoring function.\n",
    "\n",
    "Currently, we have two different options implemented for $s$:\n",
    "* `jaccard`: jaccard similarity (treating $n_1[w]$ as a set)\n",
    "* `adapted-ktau`: an adapted version of Kendall-Tau that computes the similarity of two partial orders. See `adaptedKendallTau.ipynb` for more details.\n",
    "\n",
    "We intend to look into other similarity scores between partial orders as described in the thesis *Metric methods for analyzing partially ranked data* by Douglas Critchlow (November 1984). However, as you will see in the results below, the adapted Kendall-Tau method may do a sufficiently good job for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(embed.compare_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(embed.parallel_compare_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WARNING: the parallel version will hang with Jaccard as it is already super fast and there is likely a race somewhere in Pool(). Only use it with adapted Kendall-Tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "jaccard_comparison = embed.compare_neighbors(neighbors_1[0], neighbors_2[0], how='jaccard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jaccard_df = pd.DataFrame(list(zip(*jaccard_comparison)), columns=['label', 'similarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "akt_comparison = embed.parallel_compare_neighbors(neighbors_1[0], neighbors_2[0], how='adapted-ktau', n_proc=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "akt_df = pd.DataFrame(list(zip(*akt_comparison)), columns=['label', 'similarity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define the overall neighbourhood similarity score to be the mean of the local neighbourhood similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Jaccard neighbourhood similarity score: {}\".format(scipy.mean(jaccard_df['similarity'])))\n",
    "print(\"Adapted Kendall-Tau neighbourhood similarity score: {}\".format(scipy.mean(akt_df['similarity'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want to see what's going on under the hood, as well as finding out what accounts for the difference in these neighbourhood similarity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the distributions of the neighbourhood similarity scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we want to know if taking mean of the local similary scores is a reasonable thing to do. Let's actually look at the distribution of the similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = scipy.stats.describe(jaccard_df['similarity'])\n",
    "print(\"Jaccard neighbourhood similarity stats:\\n mean:{}\\n variance:{}\".format(stats.mean, stats.variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "plt.title(\"Histogram of Jaccard Local Neighbourhood Similarity Scores\")\n",
    "sns.distplot(jaccard_df['similarity'], axlabel='Similarity', kde=True, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats = scipy.stats.describe(akt_df['similarity'])\n",
    "print(\"Adapted Kendall-Tau neighbourhood similarity stats:\\n mean:{}\\n variance:{}\".format(stats.mean, stats.variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "plt.title(\"Histogram of adapted Kendall-Tau Neighbourhood Similarity Scores\")\n",
    "sns.distplot(akt_df['similarity'], axlabel='Similarity', kde=True, bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram of adapted Kendall-Tau neighbourhood similarity scores is a lot smoother, whereas the discrete nature of Jaccard similarity has a strong effect on the overall distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What accounts for the difference in jaccard and adapted Kendall-Tau similarities?\n",
    "\n",
    "First let's compute the difference between the jaccard and adapted kendall-tau neighbourhood similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = jaccard_df.merge(akt_df, on=['label'], suffixes=['_jac', '_akt'])\n",
    "df['sim_diff'] = df.similarity_akt - df.similarity_jac\n",
    "df.sort_values('sim_diff', ascending=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the relationship between the Jaccard and Kendall-Tau scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.JointGrid(x='similarity_jac', y='similarity_akt', data=df, size=10)\n",
    "g = g.plot_joint(plt.scatter, c=df['sim_diff'], s=3, cmap='viridis_r')\n",
    "plt.plot(df['similarity_jac'], df['similarity_jac'], c='black')\n",
    "g = g.set_axis_labels(ylabel='Adapted Kendall-Tau Neighbourhood Similarity', xlabel='Jaccard Neighbourhood Similarity')\n",
    "g = g.plot_marginals(sns.distplot, kde=False, bins=100)\n",
    "g = g.annotate(scipy.stats.pearsonr)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better feel for this, let's take also plot the differences between adapted Kendall-Tau and Jaccard similarities in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "N = 100\n",
    "n_samples = 50\n",
    "mOrd = []\n",
    "mOrdR = []\n",
    "mTopSh = []\n",
    "p = []\n",
    "mSh = []\n",
    "jac = []\n",
    "X = list(range(1,N+1,1))\n",
    "Z = list(range(N+1,2*N+1,1))\n",
    "\n",
    "for m in range(1,N+1,1):\n",
    "    xOrd = []\n",
    "    xOrdR = []\n",
    "    xSh = []\n",
    "    xTopSh = []\n",
    "    p.append(m/N)\n",
    "    for l in range(n_samples):\n",
    "        ## list of length N with ordered top-m in common\n",
    "        Y = X[0:m]\n",
    "        Y.extend(random.sample(Z,N-m))\n",
    "        xOrd.append(embed.adaptedKendallTau(X,Y))\n",
    "\n",
    "        ## list of length N with top-m in common\n",
    "        Y = X[0:m]\n",
    "        Y.extend(random.sample(Z,N-m))\n",
    "        random.shuffle(Y)\n",
    "        xTopSh.append(embed.adaptedKendallTau(X,Y))\n",
    "        \n",
    "        ## list of length N with m in common\n",
    "        Y = random.sample(X,m)\n",
    "        Y.extend(random.sample(Z,N-m))\n",
    "        random.shuffle(Y)\n",
    "        xSh.append(embed.adaptedKendallTau(X,Y))\n",
    "    mOrd.append(np.mean(xOrd))\n",
    "    mTopSh.append(np.mean(xTopSh))\n",
    "    mSh.append(np.mean(xSh))\n",
    "    jac.append(embed.jaccard(set(X),set(Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.JointGrid(x='similarity_jac', y='similarity_akt', data=df, size=10)\n",
    "g = g.plot_joint(plt.scatter, c=df['sim_diff'], s=3, cmap='viridis_r')\n",
    "plt.plot(jac,mSh, ls='dotted', label='Shuffle',color='blue')\n",
    "plt.plot(jac,mOrd, ls='dotted', label='Ordered',color='purple')\n",
    "plt.plot(jac,mTopSh, ls='dotted', label='Ordered',color='green')\n",
    "\n",
    "plt.xlabel(\"Jaccard similarity\")\n",
    "plt.ylabel(\"Tau similarity\")\n",
    "g = g.set_axis_labels(ylabel='Adapted Kendall-Tau Neighbourhood Similarity', xlabel='Jaccard Neighbourhood Similarity')\n",
    "g = g.plot_marginals(sns.distplot, kde=False, bins=100)\n",
    "g = g.annotate(scipy.stats.pearsonr)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the additional lines represent the effect of ordering on the adapted Kendall-Tau similarity metric:\n",
    "\n",
    "* purple: the common elements are all in order at the beginning of the list\n",
    "* green: the common elements are all at the beginning of the list (shuffled)\n",
    "* blue: the common elements are in random order\n",
    "\n",
    "This suggests that the common elements are mostly at the beginning of the list, and are mostly in the correct order. \n",
    "\n",
    "The colorbar represents the value of the adapted Kendall-Tau similarity minus the Jaccard similarity. The colouring suggests that the largest differences lie in the range where the purple and blue lines differ the most.\n",
    "\n",
    "It also looks like there may be a normal distribution across the color bar. Let's check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "plt.title(\"Distribution of adapted Kendall-Tau minus Jaccard Neighbourhood Similarity Scores\")\n",
    "sns.distplot(df['sim_diff'], axlabel='Difference in Similarity', kde=False, norm_hist=False, bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a smooth looking plot! Plus it's worth noting that the adapted Kendall-Tau similarity is almost always greater than the Jaccard similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats = scipy.stats.describe(df['sim_diff'])\n",
    "print(\"Adapted Kendall-Tau minus Jaccard neighbourhood similarity stats:\\n mean: {}\\n variance: {}\\n minimum: {}\\n maximum: {}\".format(stats.mean, stats.variance, *stats.minmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the choice of k affect the scores?\n",
    "So far we haven't looked at the effect that the size of the neighbourhood that we're comparing is having, or how this affects the Jaccard vs. adapted Kendall-Tau similarity scores.\n",
    "\n",
    "First, let's take a look at the words for which Jaccard and adapted Kendall-Tau differ the most, and compare the scores as we vary over the number of neighbours we're comparing, that is, over k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words whose adapted Kendall-Tau and Jaccard neighbourhood similarities differ the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_similarities_varying_k(label, n1, n2):\n",
    "\n",
    "    similarities = []\n",
    "    for k in range(2, len(n1)):\n",
    "        akt_k = embed.adaptedKendallTau(n1[1:k], n2[1:k])\n",
    "        jac_k = embed.jaccard(set(n1[1:k]), set(n2[1:k]))\n",
    "        similarities.append([k, jac_k, akt_k])\n",
    "\n",
    "    similarities_df = pd.DataFrame(similarities, columns=['k', 'similarity_jac', 'similarity_akt'])\n",
    "\n",
    "    plt.plot(similarities_df['k'], similarities_df['similarity_jac'], label='Jac')\n",
    "    plt.plot(similarities_df['k'], similarities_df['similarity_akt'],label='aKT')\n",
    "    plt.legend()\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Similarity')\n",
    "\n",
    "    plt.title('Neighbourhood similarity of \"{}\" as k varies'.format(label));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 60))\n",
    "for i, label in enumerate(df.sort_values('sim_diff', ascending=False)[:6].label):\n",
    "    n1 = neighbors_1[0][label]\n",
    "    n2 = neighbors_2[0][label]\n",
    "    plt.subplot(6, 2, i+1)\n",
    "    compare_similarities_varying_k(label, n1, n2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These pictures are all roughly the same. There is some initial instability, as both measures are extremely sensitive to having different neighbours when the k value is low. There is a value of $k$ after which the neighbors mostly differ as we see the Jaccard similarity score steadily declining. However, the neighbors that have been added after the initial instability were all added in roughly the same order. \n",
    "\n",
    "It is worth noting that Jaccard similarity seems to give a stronger indication that we've left a similar \"local neighborhood\" than adapted Kendall-Tau does. Still, there is still a noticeable inflection point on the Kendall-Tau similarity graphs. We will return to this idea of the choosing $k$, and detecting the \"local neighbourhood\" structure later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is this typical?\n",
    "\n",
    "Of course, these were the words that exhibited the most difference between the two different similarity scores. Let's take a random sample of 6 words near the mean difference between Kendall-Tau and Jaccard similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "most_typical_filter = (stats.mean - stats.variance < df.sim_diff) & (df.sim_diff < stats.mean + stats.variance)\n",
    "sample = random.sample(list(np.where(most_typical_filter)[0]), 6)\n",
    "plt.figure(figsize=(25, 80))\n",
    "for i, label in enumerate(df.iloc[sample].label):\n",
    "    n1 = neighbors_1[0][label]\n",
    "    n2 = neighbors_2[0][label]\n",
    "    plt.subplot(10, 2, i+1)\n",
    "    compare_similarities_varying_k(label, n1, n2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This main difference that stands out with these examples from ones where the aKT minus Jaccard is the greatest, is that there is no pronounced drop off in the Jaccard score at some point. The difference between the two scores is almost constant beyond the initial volatility. \n",
    "\n",
    "Let's check the other extreme for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 60))\n",
    "for i, label in enumerate(df.sort_values('sim_diff', ascending=True)[:6].label):\n",
    "    n1 = neighbors_1[0][label]\n",
    "    n2 = neighbors_2[0][label]\n",
    "    plt.subplot(6, 2, i+1)\n",
    "    compare_similarities_varying_k(label, n1, n2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In almost all these cases, we see that the Jaccard similarity is steadily increasing. In these cases, the bias of adapted Kendall-Tau towards matching earlier elements (and getting them in the same order) means that Jaccard increases more than adapted Kendall-Tau as new elements that match are added. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of varying k (with a single comparison)\n",
    "\n",
    "In this particular example, where two fairly similar embeddings are being compared as long as k is great enough (say at least 20), both the Jaccard and adapted Kendall-Tau similarity scores are fairly stable. This is typical of all comparisons, and we'll see more evidence for this later when we do pairwise comparisons across embedding techniques while varying k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparisons of neighbourhood similarity accross embedding techniques\n",
    "\n",
    "Recall that the goal was to compare the similarity between different text embedding techniques. We used the Yelp dataset for all the comparisons. Ultimately we want to do the pairwise comparisons between:\n",
    "* PMI+SVD (factorization of the pointwise mutual information (PMI) matrix via truncated SVD)\n",
    "* fastext\n",
    "* eigenwords\n",
    "* GloVe\n",
    "* TODO: word2vec\n",
    "\n",
    "We'll do this by taking 5 runs of each embedding technique, computing the 100 nearest neighbours using cosine similarity, and then computing the neighbour similarity scores using Jaccard and Kendall-Tau. We'll only using cosine similarity here, as there is no pronounced difference in using other metrics and cosine is the generally accepted default. See another notebook referenced at the top for digging into the differences that come from using different k-nearest neighbour metrics. \n",
    "\n",
    "Since it's relatively slow to generate this many pairwise comparisons, we'll simply read in some existing comparisons. However, this can be done using `embed.get_pairwise_comparisons` and `embed.parallel_get_pairwise_comparisons` as in the notebook referenced at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "jac_comparisons, jac_metadata = embed.read_comparisons(\"25x25_cosine_text_workshop_jac\", data_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "akt_comparisons, akt_metadata = embed.read_comparisons(\"25x25_cosine_text_workshop_akt\", data_path=data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a comparison, we'll want to take a look at the similarity scores; `embed.aggregate_comparison_stats` does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(embed.aggregate_comparison_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "jac_results, jac_results_mean, jac_results_variance = embed.aggregate_comparison_stats(jac_comparisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "akt_results, akt_results_mean, akt_results_variance = embed.aggregate_comparison_stats(akt_comparisons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard Similarity Scores\n",
    "First look at the Jaccard scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create simplified labels for the upcoming plots (and replace minkowski by euclidean)\n",
    "def simplified_matrix_labels(metadata, knn=True):\n",
    "    matrix_labels = []\n",
    "    label_filter = []\n",
    "    for meta in metadata:\n",
    "        # manhattan and l1 are the same metric, so drop manhattan, euclidean, l2, and minkowski are the same\n",
    "        if 'manhattan' in meta['k-nn Metric'] or 'hamming' in meta['k-nn Metric'] or 'euclidean' in meta['k-nn Metric'] or 'l2' in meta['k-nn Metric']:\n",
    "            label_filter.append(False)\n",
    "        else:\n",
    "            label_filter.append(True)\n",
    "            if not meta['Other Information']:\n",
    "                matrix_labels.append((meta['Embedding Algorithm'], '', 'knn: ' + meta['k-nn Metric'], meta['Run Number']))\n",
    "            elif 'arpack' in meta['Other Information']:\n",
    "                matrix_labels.append((meta['Embedding Algorithm'], 'arpack', 'knn:' + meta['k-nn Metric'], meta['Run Number']))\n",
    "            elif 'randomized' in meta['Other Information']:\n",
    "                matrix_labels.append((meta['Embedding Algorithm'], 'randomized', 'knn: ' + meta['k-nn Metric'], meta['Run Number']))\n",
    "            elif 'cosine' in meta['Other Information']:\n",
    "                matrix_labels.append((meta['Embedding Algorithm'], 'cosine', 'knn: ' + meta['k-nn Metric'], meta['Run Number']))\n",
    "            else:\n",
    "                matrix_labels.append((meta['Embedding Algorithm'], '', 'knn: ' + meta['k-nn Metric'], meta['Run Number']))\n",
    "\n",
    "    for i, label in enumerate(matrix_labels):\n",
    "        if 'minkowski' in label[2]:\n",
    "            matrix_labels[i] = (label[0], label[1], label[2].replace('minkowski', 'euclidean'), label[3])\n",
    "\n",
    "    # create a permutation to group like with like\n",
    "    perm_to_sort = [matrix_labels.index(tup) for tup in sorted(matrix_labels)]\n",
    "\n",
    "    # simplify the labels for easier reading\n",
    "    if knn:\n",
    "        simple_labels = [(x, y, z) for x, y, z, _ in np.array(matrix_labels)[perm_to_sort]]\n",
    "    else:\n",
    "        simple_labels = [(x, y) for x, y, _, _ in np.array(matrix_labels)[perm_to_sort]]\n",
    "        \n",
    "    return simple_labels, label_filter, perm_to_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_labels(labels):\n",
    "    return [' '.join(x) for x in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jac_labels, jac_label_filter, jac_perm_to_sort = simplified_matrix_labels(jac_metadata, knn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 12))\n",
    "plt.title(\"Global Jaccard Similarity Scores\")\n",
    "sns.heatmap(jac_results_mean[jac_label_filter,][:,jac_label_filter][jac_perm_to_sort,][:,jac_perm_to_sort], vmin=0, cmap='Blues',\n",
    "            xticklabels=display_labels(jac_labels), yticklabels=display_labels(jac_labels));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness, let's check the variance as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(jac_results_variance[jac_label_filter,][:,jac_label_filter][jac_perm_to_sort,][:,jac_perm_to_sort], cmap='Blues',\n",
    "            xticklabels=display_labels(jac_labels), yticklabels=display_labels(jac_labels));\n",
    "plt.title(\"Jaccard Similarity Variance\", loc='center');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance between scores of neighbourhoods is extremely low. We won't check it again as it is typically very low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapted Kendall-Tau Similarity Scores\n",
    "Next up, adapted Kendall-Tau scores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "akt_labels, akt_label_filter, akt_perm_to_sort = simplified_matrix_labels(akt_metadata, knn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 12))\n",
    "plt.title(\"Global Adapted Kendall-Tau Similarity Scores\")\n",
    "sns.heatmap(akt_results_mean[akt_label_filter,][:,akt_label_filter][akt_perm_to_sort,][:,akt_perm_to_sort], vmin=0,\n",
    "            cmap='Blues', xticklabels=display_labels(akt_labels), yticklabels=display_labels(akt_labels));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every embedding techniqe seems to be similar to itself, and not at all similar to the other techniques. However, out of all the comparisons across techniques, eigenwords and PMI+SVD are the most similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests that PMI+SVD using arpack and eigenwords are the most stable, with PMI+SVD randomized somewhat less stable, and fasttext is the most unstable, but is still comparable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How about the difference between the two different similarity scores?\n",
    "\n",
    "The adapted Kendall-Tau similarity score is higher than the Jaccard. Let's see how much they differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "sns.heatmap(akt_results_mean[akt_label_filter,][:,akt_label_filter][akt_perm_to_sort,][:,akt_perm_to_sort]-jac_results_mean[jac_label_filter,][:,jac_label_filter][jac_perm_to_sort,][:,jac_perm_to_sort], vmin=0,\n",
    "               vmax=1, cmap='Blues', xticklabels=display_labels(akt_labels), yticklabels=display_labels(akt_labels));\n",
    "plt.title(\"Difference of Jaccard and Adapted Kendall-Tau Similarity Scores\", loc='center');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Maximum difference between the similarity scores: {}\".format(np.max(akt_results_mean[akt_label_filter,][:,akt_label_filter][akt_perm_to_sort,][:,akt_perm_to_sort]-jac_results_mean[jac_label_filter,][:,jac_label_filter][jac_perm_to_sort,][:,jac_perm_to_sort])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the Kendall-Tau score is greater than the Jaccard score. This suggests that within the neighbours that match, order is preserved enough to contribute noticeably to the Kendall-Tau score. \n",
    "\n",
    "Since the Jaccard takes about 30x longer to compute than adapted Kendall-Tau (serially) and is quite fast, it may be worth simply using Jaccard by default in general for computing a neighbourhood similarity score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens as k varies?\n",
    "In the single comparison situation, we saw that Jaccard was more sensitive in the beginning than adapted Kendall-Tau, and it is also more sensitive to \"leaving a local neighbourhood\". When we did the pairwise comparison above, we only used k=100. Let's check the effect on these pairwise comparisons as k varies. \n",
    "\n",
    "There are two main questions here: \n",
    "* How local is local for the various methods? What size of k is too small? What size of k is too large?\n",
    "* What accounts for the difference between adapted Kendall-Tau and Jaccard similarity as k varies? Do we see the same picture as we saw for individual words above on a global level, where we match for a while and then start adding new elements that don't match once we get out of the local neighbourhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Helper function to select off indices that we want for various stories and pictures\n",
    "def get_subselected_indices(alg, knn_metric, labels):\n",
    "    value_filter = [alg in \" \".join(label) and knn_metric in label[2] for label in labels]\n",
    "    return np.where(value_filter)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity scores as k-varies comparing runs of an algorithm against itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "algs = ['randomized', 'arpack', 'eigenwords', 'fasttext', 'GloVe']\n",
    "knn_metrics = ['cosine']\n",
    "similarity_metrics = ['akt', 'jac']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "max_k = 100\n",
    "\n",
    "self_labels = {}\n",
    "self_scores = {}\n",
    "self_comparisons = {}\n",
    "filenames = []\n",
    "for alg in algs:\n",
    "    for knn_metric in knn_metrics:\n",
    "        for similarity_metric in similarity_metrics:\n",
    "            filename = \"k_{}_comparisons_{}_{}_{}\".format(similarity_metric, alg, knn_metric, max_k)\n",
    "            filenames.append(filename)\n",
    "            \n",
    "for filename in tqdm.tqdm(filenames):\n",
    "        comparisons, metadata = embed.read_comparisons(filename, data_path=data_path)\n",
    "        _, score, _ = embed.aggregate_comparison_stats(comparisons, max_k=max_k)\n",
    "        simple_labels, label_filter, perm_to_sort = simplified_matrix_labels(metadata, knn=True)\n",
    "        perm_score = score[label_filter,][:,label_filter][perm_to_sort,][:,perm_to_sort]\n",
    "        self_labels[filename] = simple_labels\n",
    "        self_scores[filename] = perm_score\n",
    "        self_comparisons[filename] = comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "knn_metric = 'cosine'\n",
    "size = 6\n",
    "width = 2 # number of plots to use across the screen\n",
    "\n",
    "plt.figure(figsize=(20,40))\n",
    "\n",
    "# color akt and jaccard differently\n",
    "colors = {}\n",
    "colors['akt'] = sns.color_palette(\"bright\", 10)\n",
    "colors['jac'] = sns.color_palette(\"husl\", 10)\n",
    "\n",
    "plot_num = 0\n",
    "for alg in algs:\n",
    "    plot_num += 1\n",
    "    plt.subplot(int(size/width)+width, width, plot_num)\n",
    "    for similarity_metric in similarity_metrics:\n",
    "        filename = \"k_{}_comparisons_{}_{}_{}\".format(similarity_metric, alg, knn_metric, max_k)\n",
    "        indices = get_subselected_indices(alg, knn_metric, self_labels[filename])\n",
    "        color_index = 0\n",
    "        for m in indices:\n",
    "            for n in indices:\n",
    "                if m < n:\n",
    "                    color = colors[similarity_metric][color_index]\n",
    "                    plt.plot(self_scores[filename][m, n], label=\"{}: {} vs. {}\".format(similarity_metric, m, n),\n",
    "                             c=color)\n",
    "                    color_index += 1\n",
    "                    plt.ylim(ymin=0.5, ymax = 1.01)\n",
    "\n",
    "    plt.title(\"{} with knn metric {}\".format(alg, knn_metric))\n",
    "    plt.legend()\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Similarity')\n",
    "    plt.xlim(xmin=0, xmax=max_k-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that in each case, once k is large enough, say k=20, the neighbourhood similarity score is very stable, and in most cases slightly increasing with k. Furthermore, there is a constant difference between the Jaccard and adapted Kendall-Tau scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Between algorithm comparisons while varying k\n",
    "\n",
    "Now, all of these embedding techniques so far are fairly stable against themselves. Let's see what happens when we compare embeddings across techniques, as those embeddings show a lot more variation.\n",
    "\n",
    "Here, we'll only take one embedding per algorithm, as all the algorithms are fairly stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "max_k = 100\n",
    "\n",
    "across_labels = {}\n",
    "across_scores = {}\n",
    "across_comparisons = {}\n",
    "\n",
    "knn_metric = 'cosine'\n",
    "for similarity_metric in similarity_metrics:\n",
    "    filename = \"k_{}_comparisons_across_algs_{}_{}\".format(similarity_metric, knn_metric, max_k)\n",
    "    #print(filename)\n",
    "    comparisons, metadata = embed.read_comparisons(filename, data_path=data_path)\n",
    "    _, score, _ = embed.aggregate_comparison_stats(comparisons, max_k=max_k)\n",
    "    simple_labels, label_filter, perm_to_sort = simplified_matrix_labels(metadata, knn=False)\n",
    "    perm_score = score[label_filter,][:,label_filter][perm_to_sort,][:,perm_to_sort]\n",
    "    across_labels[filename] = simple_labels\n",
    "    across_scores[filename] = perm_score\n",
    "    across_comparisons[filename] = comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "knn_metric = 'cosine'\n",
    "\n",
    "size = 2\n",
    "width = 2 # number of plots to use across the screen\n",
    "\n",
    "plt.figure(figsize=(15,20))\n",
    "plot_num = 0\n",
    "\n",
    "for similarity_metric in similarity_metrics:\n",
    "    plot_num += 1\n",
    "    plt.subplot(int(size/width)+width, width, plot_num)\n",
    "    filename = \"k_{}_comparisons_across_algs_{}_{}\".format(similarity_metric, knn_metric, max_k)\n",
    "\n",
    "    indices = range(len(across_scores[filename]))\n",
    "    color_index = 0\n",
    "    for m in indices:\n",
    "        for n in indices:\n",
    "            if m < n:\n",
    "                color = sns.color_palette(\"bright\", 10)[color_index]\n",
    "                plt.plot(across_scores[filename][m, n], \n",
    "                         label=\"{} vs. {}\".format(' '.join(across_labels[filename][m]),\n",
    "                                                  ' '.join(across_labels[filename][n])),\n",
    "                         c=color)\n",
    "                color_index += 1\n",
    "    plt.title(\"Across algorithm comparisons as k varies with {}\".format(similarity_metric))\n",
    "    plt.legend()\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Similarity')\n",
    "\n",
    "    plt.xlim(xmin=0, xmax=max_k-1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, once k is large enough, say k=20, the neighbourhood similarity score is very stable, and again there is a constant difference between the Jaccard and adapted Kendall-Tau scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Comparing embeddings by using similarity of the k-nearest neighbours is an effective way to test the local similarity of different embeddings.\n",
    "\n",
    "It is stable across k-values as long as k is \"large enough\" (say larger than 20). In particular, there is no noticeable effect to \"leaving a local neighbourhood\" within the range of k=20 to k=100. \n",
    "\n",
    "Furthermore, the adapted Kendall-Tau and Jaccard similarity scores are highly correlated, and their difference is constant, usually between 0 and 0.25. Given this, since Jaccard is considerably faster (30x in the current implementation), we would suggest using Jaccard unless you would like to compare the two scores, which will give information as to how well the order of the neighbours is preserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
